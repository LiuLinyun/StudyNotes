# 集成学习

构建并结合多个学习器来完成学习任务。

## AdaBoost

Boosting是一族将弱学习器提升为强学习器的算法。工作机制为：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续得到更多关注，然后基于调整后的样本分布来训练下个基学习器；直到基学习器数目达到指定的数目 $T$，最终将这 $T$ 个基学习器进行加权结合。

AdaBoost算法：
______________________
**输入**：训练集 $D=\{(\boldsymbol x_1,y_1),(\boldsymbol x_2,y_2),\dots,(\boldsymbol x_m, y_m) \}$
$\quad \quad$ 基学习器算法 $\mathfrak L$
$\quad \quad$ 训练轮数 $T$
**过程**：

1. $\mathcal D_1(\boldsymbol x) = 1/m$;
2. **for** $t=1,2,\dots,T$ **do**
3. $\quad$ $h_t = \mathfrak L(D, \mathcal D_t)$;
4. $\quad$ $\epsilon_t = P_{\boldsymbol x \sim \mathcal D_t}(h_t(\boldsymbol x) \neq f(\boldsymbol x))$;
5. $\quad$ **if** $\epsilon_t > 0.5$ **then break**
6. $\quad$ $\alpha_t = \frac 1 2 \ln \left(\frac{1-\epsilon_t}{\epsilon_t} \right)$；
7. $\quad$ $\begin{aligned} \mathcal D_{t+1}(\boldsymbol x) & = \frac {\mathcal D_t(\boldsymbol x)} {Z_t} \times \begin{cases} \exp (-\alpha_t), & \quad h_t(\boldsymbol x) = f(\boldsymbol x) \\ \exp(\alpha_t), & \quad h_t(\boldsymbol x) \neq f(\boldsymbol x) \end{cases} \\ & = \frac{\mathcal D_t(\boldsymbol x)\exp(-\alpha_t f(\boldsymbol x)h_t(\boldsymbol x))} {Z_t} \end{aligned}$
8. **end for**

**输出**：$H(\boldsymbol x) = \text{sign} \left(\sum_{t=1}^T{\alpha_t h_t(\boldsymbol x)} \right)$
______________________
其中：$\mathcal D(\boldsymbol x)$ 表示样本 $(\boldsymbol x,y)$ 对应在分布中对应的权重；


## Bagging

## 随机森林

## XGBoost

## GDBT